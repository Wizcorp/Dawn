####
## Output descriptions:
##

## match tag=debug.** and dump to console
<match debug.**>
  @type stdout
</match>

####
## Source descriptions:
##

## built-in TCP input
## @see http://docs.fluentd.org/articles/in_forward
<source>
  @type forward
</source>

# HTTP input
# POST http://localhost:8888/<tag>?json=<json>
# POST http://localhost:8888/td.myapp.login?json={"user"%3A"me"}
# @see http://docs.fluentd.org/articles/in_http
<source>
  @type http
  port 8888
</source>

# Parse our custom rsyslog output
<source>
  @type syslog
  port 5140
  bind 127.0.0.1
  tag syslog
  format /^(?<time>[^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$/
  time_format %Y-%m-%dT%H:%M:%S.%L%z
</source>

# Then read the json data stored in the message
<filter syslog.**>
  @type parser
  format json
  key_name message
  inject_key_prefix parsed.
  reserve_data true
</filter>

# Finally convert this parsed data if available, and strip out all the useless stuff
<filter>
  @type record_transformer
  enable_ruby true
  <record>
    @timestamp ${record["time"] || Time.now.strftime('%Y-%m-%dT%H:%M:%S.%L%z')}
    host ${record["host"] || hostname}
    message ${record["parsed.MESSAGE"] || record["message"]}
    container_name ${record["parsed.CONTAINER_NAME"]}
    container_id ${record["parsed.CONTAINER_ID"]}
    pid ${record["parsed.PID"]}
  </record>
  renew_record true
  keep_keys @timestamp,message,container_name,container_id,pid,host
</filter>

## forward everything else to elasticsearch/kibana
<match>
  @type elasticsearch
  host {{ group_ipv4.monitor[0] }}
  logstash_format true
  reconnect_on_error true
  time_key_format %Y-%m-%dT%H:%M:%S.%L%z
#  template_name fluentd
#  template_file /etc/td-agent/conf.d/index.tpl.json
  flush_interval 10s
</match>
